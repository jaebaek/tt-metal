# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0


import torch
import ttnn
import tt_lib

from ttnn.model_preprocessing import preprocess_model, preprocess_conv2d, fold_batch_norm2d_into_conv2d

from tests.ttnn.utils_for_testing import assert_with_pcc
from models.utility_functions import skip_for_wormhole_b0

from models.experimental.functional_yolov4.reference.yolov4 import Yolov4
from models.experimental.functional_yolov4.tt.ttnn_yolov4 import TtYolov4

import tests.ttnn.integration_tests.yolov4.custom_preprocessor_d1 as D1
import tests.ttnn.integration_tests.yolov4.custom_preprocessor_d2 as D2
import tests.ttnn.integration_tests.yolov4.custom_preprocessor_d3 as D3
import tests.ttnn.integration_tests.yolov4.custom_preprocessor_d4 as D4
import tests.ttnn.integration_tests.yolov4.custom_preprocessor_d5 as D5
import tests.ttnn.integration_tests.yolov4.custom_preprocessor_neck as neck
import tests.ttnn.integration_tests.yolov4.custom_preprocessor_head as head


def create_custom_preprocessor(device):
    def custom_preprocessor(model, name, ttnn_module_args):
        parameters = {}
        parameters["downsample1"] = D1.custom_preprocessor(
            device, model.downsample1, name, ttnn_module_args["downsample1"]
        )
        parameters["downsample2"] = D2.custom_preprocessor(
            device, model.downsample2, name, ttnn_module_args["downsample2"]
        )
        parameters["downsample3"] = D3.custom_preprocessor(
            device, model.downsample3, name, ttnn_module_args["downsample3"]
        )
        parameters["downsample4"] = D4.custom_preprocessor(
            device, model.downsample4, name, ttnn_module_args["downsample4"]
        )
        parameters["downsample5"] = D5.custom_preprocessor(
            device, model.downsample5, name, ttnn_module_args["downsample5"]
        )
        parameters["neck"] = neck.custom_preprocessor(device, model.neck, name, ttnn_module_args["neck"])
        parameters["head"] = head.custom_preprocessor(device, model.head, name, ttnn_module_args["head"])
        return parameters

    return custom_preprocessor


import pytest


# @pytest.mark.parametrize("device_l1_small_size", [32768], indirect=True)
@pytest.mark.parametrize("device_params", [{"l1_small_size": 32768}], indirect=True)
@skip_for_wormhole_b0()
def test_downsample1(device, reset_seeds, model_location_generator):
    model_path = model_location_generator("models", model_subdir="Yolo")
    weights_pth = str(model_path / "yolov4.pth")
    state_dict = torch.load(weights_pth)
    ds_state_dict = {
        k: v
        for k, v in state_dict.items()
        if (k.startswith(("down1.", "down2.", "down3.", "down4.", "down5.", "neek.", "head.")))
    }
    #    ds_state_dict_head = {k: v for k, v in state_dict.items() if k.startswith("head.")}
    torch_model = Yolov4()
    #    torch_model_head_dict = {k: v for k, v in torch_model.state_dict().items() if k.startswith("head.")}
    #    new_state_dict_head = torch_model_head_dict
    #    old_state_dict_head = torch_model_head_dict.copy()
    #    torch_model_named_params_head = {k: v for k, v in torch_model.named_parameters() if k.startswith("head.")}
    #    sizes = {}
    #    for name, param in torch_model_named_params_head.items():
    #        sizes[name] = param.size()
    #
    #    # Iterate over the parameters of the model
    #    for name, param in torch_model_named_params_head.items():
    #        # print("name: ", name)
    #        found_match = False
    #        # Check if the parameter has a corresponding key in the loaded state dictionary
    #        for ds_name, ds_param in ds_state_dict_head.items():
    #            if param.size() == ds_param.size():
    #                new_state_dict_head[name] = ds_param
    #                found_match = True
    #                # print("found_match: ", found_match)
    #                # print("new_state_dict[name] == old_state_dict[name]: ", new_state_dict[name] == old_state_dict[name])
    #                break  # Found a match, move to the next parameter
    #
    #    new_state_dict = {}
    #    ds_state_dict_without_head = {k: v for k, v in ds_state_dict.items() if not k.startswith("head.")}
    #    torch_model_without_head = {k: v for k, v in torch_model.state_dict().items() if not k.startswith("head.")}
    #    keys = [name for name, parameter in torch_model_without_head.items()]
    #    values = [parameter for name, parameter in ds_state_dict_without_head.items()]
    #    names = [name for name, parameter in ds_state_dict_without_head.items()]
    #
    #    print("Key len: ", len(keys), len(values))
    #    for i in range(len(keys)):
    #        new_state_dict[keys[i]] = values[i]
    #    new_state_dict.update(new_state_dict_head)
    # print("Full yolo weights: ",new_state_dict.keys())

    new_state_dict = {}
    keys = [name for name, parameter in torch_model.state_dict().items()]
    values = [parameter for name, parameter in ds_state_dict.items()]

    for i in range(len(keys)):
        new_state_dict[keys[i]] = values[i]

    torch_model.load_state_dict(new_state_dict)
    torch_model.eval()

    torch_input_tensor = torch.randn(1, 3, 320, 320)  # Batch size of 1, 128 input channels, 160x160 height and width
    torch_output_tensor1, torch_output_tensor2, torch_output_tensor3 = torch_model(torch_input_tensor)
    reader_patterns_cache = {}
    parameters = preprocess_model(
        initialize_model=lambda: torch_model,
        run_model=lambda model: model(torch_input_tensor),
        custom_preprocessor=create_custom_preprocessor(device),
        reader_patterns_cache=reader_patterns_cache,
        device=device,
    )

    ttnn_model = TtYolov4(device, parameters)

    # Tensor Preprocessing
    #
    input_shape = torch_input_tensor.shape
    input_tensor = torch.permute(torch_input_tensor, (0, 2, 3, 1))

    input_tensor = input_tensor.reshape(
        input_tensor.shape[0], 1, input_tensor.shape[1] * input_tensor.shape[2], input_tensor.shape[3]
    )
    input_tensor = ttnn.from_torch(input_tensor, dtype=ttnn.bfloat8_b, layout=ttnn.TILE_LAYOUT)
    output_tensor1, output_tensor2, output_tensor3 = ttnn_model(device, input_tensor)

    #
    # Tensor Postprocessing
    #
    output_tensor1 = ttnn.to_torch(output_tensor1)
    output_tensor1 = output_tensor1.reshape(1, 40, 40, 255)
    output_tensor1 = torch.permute(output_tensor1, (0, 3, 1, 2))
    output_tensor1 = output_tensor1.to(torch_input_tensor.dtype)

    output_tensor2 = ttnn.to_torch(output_tensor2)
    output_tensor2 = output_tensor2.reshape(1, 20, 20, 255)
    output_tensor2 = torch.permute(output_tensor2, (0, 3, 1, 2))
    output_tensor2 = output_tensor2.to(torch_input_tensor.dtype)

    output_tensor3 = ttnn.to_torch(output_tensor3)
    output_tensor3 = output_tensor3.reshape(1, 10, 10, 255)
    output_tensor3 = torch.permute(output_tensor3, (0, 3, 1, 2))
    output_tensor3 = output_tensor3.to(torch_input_tensor.dtype)

    assert_with_pcc(torch_output_tensor1, output_tensor1, pcc=0.99)  # PCC = 0.9997609248813403
    assert_with_pcc(torch_output_tensor2, output_tensor2, pcc=0.99)  # PCC = 0.0 as nan
    assert_with_pcc(torch_output_tensor3, output_tensor3, pcc=0.99)  # PCC = -0.010033306229600303
